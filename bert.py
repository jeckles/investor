# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eu5oTD1fMxmpHAII3lYOBvLz2YBjthEr
"""

from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures

model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

model.summary()

import tensorflow as tf
import pandas as pd

df = pd.read_csv('./processed_dataset.csv')
df = df[['text', 'sentiment']]
df = df.dropna()
df.head()

splitpoint = int(len(df) * .7)
train = df.iloc[:splitpoint,:]
test = df.iloc[splitpoint:,:]

def convert_to_input_examples(train, test, data_column, label_column):
  # train_input_examples = train.apply(lambda x: print(x))#InputExample(guid=None, text_a=x[data_column], text_b=None, label=x[label_column], axis=1))
  train_input_examples = []
  for index, row in train.iterrows():
    train_input_examples.append(InputExample(guid=None, text_a=row[data_column], text_b=None, label=row[label_column]))

  test_input_examples = []
  for index, row in test.iterrows():
    test_input_examples.append(InputExample(guid=None, text_a=row[data_column], text_b=None, label=row[label_column]))

  return train_input_examples, test_input_examples

def convert_examples_to_dataset(examples, tokenizer, max_length=128):
  features = []

  for example in examples:
    input_dict = tokenizer.encode_plus(
        example.text_a,
        add_special_tokens=True,
        max_length=max_length,
        return_token_type_ids=True,
        return_attention_mask=True,
        pad_to_max_length=True,
        truncation=True
    )

    input_ids, token_type_ids, attention_mask = (input_dict['input_ids'], 
        input_dict['token_type_ids'], input_dict['attention_mask'])
    
    features.append(
        InputFeatures(
            input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=example.label
        )
    )
    

  def gen():
    for f in features:
      yield (
          {
            "input_ids": f.input_ids,
            "attention_mask": f.attention_mask,
            "token_type_ids": f.token_type_ids,
          },
          f.label,
      )
  

  return tf.data.Dataset.from_generator(
        gen,
        ({"input_ids": tf.int32, "attention_mask": tf.int32, "token_type_ids": tf.int32}, tf.int64),
        (
            {
                "input_ids": tf.TensorShape([None]),
                "attention_mask": tf.TensorShape([None]),
                "token_type_ids": tf.TensorShape([None]),
            },
            tf.TensorShape([]),
        ),
    )

train_examples, test_examples = convert_to_input_examples(train, test, 'text', 'sentiment')

train_data = convert_examples_to_dataset(train_examples, tokenizer)
train_data = train_data.shuffle(100).batch(32).repeat(2)

test_data = convert_examples_to_dataset(test_examples, tokenizer)
test_data = test_data.batch(32)

model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=.01),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])

model.fit(train_data, epochs=2)